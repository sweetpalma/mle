{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1328c26f-f4c4-459e-b0e1-51208b368377",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "In our [previous MLP experiment](./03-mlp.ipynb), we saw that a neural network could extract more from averaged word embeddings than a simple Logistic Regression. However, averaging throws away crucial information - the order of words in a sentence.\n",
    "\n",
    "This time, we are going to dive deeper into **recurrent neural networks**. They are specifically designed to process sequences, remembering context and understanding how word order contributes to meaning. Let's see if harnessing this sequential power can push our accuracy even further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7778f81-c1de-4370-8059-7746a7f51ff3",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf07db0-756f-4ff1-bc78-e47f125eb564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10750bf10>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After the SuperFriends and Scooby Doo left the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good job.that's how i would describe this anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michael Cacoyannis has had a relatively long c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've just seen this film in a lovely air-condi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My one-line summary hints that this is not a g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>***SPOILERS*** ***SPOILERS*** After two so-so ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>Way back in 1967, a certain director had no id...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>I saw this movie with my dad. I must have been...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>During my teens or should I say prime time I w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>Paranoid Park is about Alex, a 16 year old ska...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      After the SuperFriends and Scooby Doo left the...      1\n",
       "1      good job.that's how i would describe this anim...      1\n",
       "2      Michael Cacoyannis has had a relatively long c...      1\n",
       "3      I've just seen this film in a lovely air-condi...      0\n",
       "4      My one-line summary hints that this is not a g...      1\n",
       "...                                                  ...    ...\n",
       "39995  ***SPOILERS*** ***SPOILERS*** After two so-so ...      1\n",
       "39996  Way back in 1967, a certain director had no id...      0\n",
       "39997  I saw this movie with my dad. I must have been...      1\n",
       "39998  During my teens or should I say prime time I w...      1\n",
       "39999  Paranoid Park is about Alex, a 16 year old ska...      0\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('stanfordnlp/imdb', split='train+test')\n",
    "train, test = ds.train_test_split(test_size=0.2, seed=0).values()\n",
    "display(train.to_pandas())\n",
    "\n",
    "x_train = train['text']\n",
    "y_train = train['label']\n",
    "x_test = test['text']\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554972f5-640d-45a8-b5b6-f12d663ea3b9",
   "metadata": {},
   "source": [
    "## Data Encoding\n",
    "\n",
    "In our previous notebook, we simply tokenized text and then immediately squashed it into the averaged word vectors. Our new recurrent pipeline needs to be a bit more complicated.\n",
    "\n",
    "This time, we would simply **tokenize** our sentences first - without trying to vectorize them. We might limit the vocabulary, making it more robust and accurate representation of frequently occurring words by filtering out rare or noisy terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718fb948-3b18-4c9f-acf1-fd0cb0c31c5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[1;32m      3\u001b[0m max_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m45000\u001b[39m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(num_words\u001b[38;5;241m=\u001b[39mmax_vocab, oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<oov>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_vocab = 45000\n",
    "tokenizer = Tokenizer(num_words=max_vocab, oov_token='<oov>')\n",
    "\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "display(tokenizer.texts_to_sequences(['Hello World']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b724b6-e573-483b-ac63-ba0adc323d24",
   "metadata": {},
   "source": [
    "Next, we need to **pad** our sequences. Most neural networks require input sequences to be of a uniform length. Since our sentences naturally vary, we need to bring them to the same length. \n",
    "\n",
    "This involves either truncating longer sequences or adding special tokens (usually zeros) to shorter sequences until they all reach a predetermined maximum length. Let's do a small research to determine how long our sequences usually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e645d0-2af6-4edc-bac6-f41484920ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "text_lengths = train.to_pandas()['text'].apply(lambda x: len(x.split()))\n",
    "display(text_lengths.describe(percentiles=[0.25, 0.5, 0.75, 0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1319ad8-3e32-48e9-9a34-785271308c9b",
   "metadata": {},
   "source": [
    "Taking the top quartile length seems to be a reasonable choice here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705e757-43a8-475b-bdd6-8636769f3f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "\n",
    "max_seq = 280\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen=max_seq)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=max_seq)\n",
    "\n",
    "display(x_train)\n",
    "display(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f1e4da-04dd-493c-bb4d-1b89ee6dc07d",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "\n",
    "Finally, we need to transform these padded sequences into some meaningful representation that captures their semantic relationships. That's where the **embedding layer** comes in. To build it, we may utilize our existing pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ef7df-f983-4f68-8fd4-ecbe96312d5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from huggingface_hub import snapshot_download\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_path = path.join(snapshot_download('fse/word2vec-google-news-300'), 'word2vec-google-news-300.model')\n",
    "wv = KeyedVectors.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883aa41-cc3c-4850-a70e-2630fde63690",
   "metadata": {},
   "source": [
    "Think of it as a sophisticated, trainable lookup table - for each token in our sequence, the embedding layer looks up its corresponding dense vector. This helps our neural network to capture not some random indexes, but the *semantic* meaning of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1612d0e-fc73-4675-8534-de9f591129ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedding_matrix_shape = (max_vocab, wv.vector_size)\n",
    "embedding_matrix = np.zeros(shape=embedding_matrix_shape)\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index < max_vocab:\n",
    "        if word in wv:\n",
    "            embedding_matrix[index] = wv.get_vector(word)\n",
    "        else:\n",
    "            embedding_matrix[index] = np.zeros(wv.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638abb1-d4a5-4366-9731-1389be7692a3",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c2f809-7c53-45b3-ae16-cc60ec1f12eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train_encoded = to_categorical(y_train, num_classes=len(ds.features['label'].names))\n",
    "y_test_encoded = to_categorical(y_test, num_classes=len(ds.features['label'].names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1dbbc-fa62-4355-af58-fe4f63a030f1",
   "metadata": {},
   "source": [
    "## Building and Training the Model\n",
    "\n",
    "Here comes the most interesting part. With our text now represented as sequences of dense semantic vectors (thanks to the embedding layer), we can introduce the star of this experiment - the **Long Short-Term Memory (LSTM)** layer.\n",
    "\n",
    "Unlike a simple dense layer that processes all its inputs at once, an LSTM processes each vector in our sequence one at a time. Internally, each LSTM unit contains a sophisticated set of **gates** – an input gate, a forget gate, and an output gate – along with a memory cell.\n",
    "\n",
    "These gates learn to control the flow of information, deciding what to remember from previous steps, what to discard, and what new information from the current word's vector is important enough to update its memory, allowing it to capture *context* and *dependencies* across the entire sequence.\n",
    "\n",
    "We may also make the recurrent layer **bidirectional**, wrapping it in a special helper function - this will allow the network to capture information from both past and future time steps, leading to a more comprehensive understanding of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8333ee3c-872d-4ac2-9ef6-3f0c56a40826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import set_random_seed\n",
    "from keras import layers, Sequential\n",
    "\n",
    "num_classes = len(ds.features['label'].names)\n",
    "set_random_seed(0)\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Input(shape=(max_seq,)),\n",
    "    layers.Embedding(\n",
    "        weights=[embedding_matrix],\n",
    "        input_dim=embedding_matrix_shape[0],\n",
    "        output_dim=embedding_matrix_shape[1],\n",
    "        trainable=True,\n",
    "    ),\n",
    "    layers.SpatialDropout1D(0.35),\n",
    "    layers.Bidirectional(layers.LSTM(256, dropout=0.3, return_sequences=True)),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dropout(0.6),\n",
    "    layers.Dense(num_classes, activation='softmax'),\n",
    "])\n",
    "\n",
    "display(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300effc-a4e5-4963-81ca-dd11e611310e",
   "metadata": {},
   "source": [
    "Before we start the training, we might tweak a few more things. Changing the optimizer learning rate may be a good idea - the default one might be too high when fine-tuning pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be945fe-c351-4a71-899e-20bd0ed86905",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import AdamW\n",
    "optimizer = AdamW(learning_rate=0.0001, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b191b-5984-416f-8409-cb993f800703",
   "metadata": {},
   "source": [
    "Also, we might introduce a **learning rate scheduler** called `ReduceLROnPlateau` - it automatically reduces the optimizer's learning rate when a monitored metric (like validation loss) stops improving for a specified number of epochs (patience limit). This helps the model escape local minima and fine-tune its weights more precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8647282-d4c3-4ee2-ab69-479e35677ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab0c0f-dc5a-41f0-ba5f-d6200cbd8144",
   "metadata": {},
   "source": [
    "Now, let's compile and train our final model. This time we might start using the GPU - our model is finally complex enough to leverage the parallelism it offers to speed up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844a91a-c0f1-454b-961d-ec0d16eaf829",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train_encoded, epochs=20, batch_size=64, callbacks=[reduce_lr], validation_split=0.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff4ab5f-7b61-4d05-9326-a6c22ae5083a",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff80d1-d975-4934-877f-610e21440a8b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "y_pred_probs = model.predict(x_test, verbose=False)\n",
    "y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "y_true_labels = np.argmax(y_test_encoded, axis=1)\n",
    "print(classification_report(y_true_labels, y_pred_labels, target_names=ds.features['label'].names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f887c-af46-4e72-ae68-fc85b53b14ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4.5, 3))\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda7276-fb61-44a1-a38b-cbf544ba5f1a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We achieved a final accuracy of **91%**, ultimately outperforming the linear model. It became possible by combining LSTM units with pre-trained word embeddings, enabling the model to leverage sequential information.\n",
    "\n",
    "This proves the value of sequence-aware models over simple embedding averaging for this type of task, performing competitively with the heavily optimized count vectorizer approach. Further improvements would likely require exploring more advanced architectures like transformers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
